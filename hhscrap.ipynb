{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3e805d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# get html from the response\n",
    "def get_html(url):      \n",
    "    headers = {'User-Agent': 'Chrome/142.0.7444.164'}\n",
    "    rq = requests.get(url, headers=headers)\n",
    "    print('Gettin HTML-code from ', url)\n",
    "    return rq.text\n",
    "\n",
    "\n",
    "# check if vacansy exists\n",
    "def is_empty(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    links = soup.find_all('a', class_='magritte-link___b4rEM_6-1-8 magritte-link_mode_primary___l6una_6-1-8 magritte-link_style_neutral___iqoW0_6-1-8 magritte-link_enable-visited___Biyib_6-1-8')\n",
    "    if links == []:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# get link for all job listings\n",
    "def get_all_offers_links(query, area):\n",
    "    # headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    url_base = 'https://tashkent.hh.uz/search/vacancy'\n",
    "    url_text = '?text='+query\n",
    "    url_area = '&area='+area\n",
    "    url_page = '&page='\n",
    "\n",
    "    # mark end of the list\n",
    "    page_is_not_empty = True\n",
    "\n",
    "    all_links = []\n",
    "    page = 1\n",
    "\n",
    "    while page_is_not_empty:\n",
    "        url = url_base + url_text + url_area + url_page + str(page)\n",
    "        time.sleep(.5)\n",
    "        html = get_html(url)\n",
    "        if not is_empty(html):\n",
    "            all_links = get_offers_links(html, all_links)\n",
    "            page += 1\n",
    "        else:\n",
    "            page_is_not_empty = False\n",
    "    return all_links\n",
    "\n",
    "\n",
    "# функция, которая собирает все ссылки на вакансии на странице поиска\n",
    "# принимает список, который уже может быть не пустой, возвращает дополненный список\n",
    "def get_offers_links(html, all_links):\n",
    "    # новый объект класса BeutifulSoup\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    links = soup.find_all('a', class_='magritte-link___b4rEM_6-1-8 magritte-link_mode_primary___l6una_6-1-8 magritte-link_style_neutral___iqoW0_6-1-8 magritte-link_enable-visited___Biyib_6-1-8')\n",
    "    for link in links:\n",
    "        link_parsed = link.get('href').split('?')\n",
    "        all_links.append(link_parsed[0])\n",
    "    return all_links\n",
    "\n",
    "\n",
    "# функция, которая парсит блок с ключевыми навыками и возвращает дополненный словарь, который ей дали на входе\n",
    "def parse_skills_in_offer(soup, skill_dict):\n",
    "    # находим блок с ключевыми навыками на странице\n",
    "    key_skills = soup.find_all('div', class_='magritte-tag__label___YHV-o_4-0-24')\n",
    "\n",
    "    # добавляем текст навыков в словарь\n",
    "    for skill in key_skills:\n",
    "        if skill.get_text().lower() in skill_dict:\n",
    "            skill_dict[skill.get_text().lower()] += 1\n",
    "        else:\n",
    "            skill_dict[skill.get_text().lower()] = 1\n",
    "\n",
    "    return skill_dict\n",
    "\n",
    "\n",
    "# функция, которая парсит блок с описанием вакансии и возвращает дополненный словарь, который ей дали на входе\n",
    "def parse_description_in_offer(soup, description_dict):\n",
    "    # описание вакансии\n",
    "    if not soup.find(string=\"Вакансия в архиве\"):\n",
    "        description = soup.find('div', class_='vacancy-description')\n",
    "        # оставим только текст без тегов\n",
    "        text = ''.join(description.findAll(text=True))\n",
    "        # почистим текст от знаков препинания\n",
    "        for elem in ('.',',',';',':','\"'):\n",
    "            if elem in text:\n",
    "                text = text.replace(elem, ' ')\n",
    "        # проверим каждое слово и занесем его в словарь\n",
    "        for word in text.split(' '):\n",
    "            if word.lower() in description_dict:\n",
    "                description_dict[word.lower()] += 1\n",
    "            else:\n",
    "                description_dict[word.lower()] = 1\n",
    "\n",
    "    return description_dict\n",
    "\n",
    "\n",
    "# функция, которая парсит основные регионы со страницы https://hh.ru/search/vacancy\n",
    "# и сохраняет название региона и его код для GET запроса в файл\n",
    "# функция нужна для себя - чтобы знать, какой код региона использовать\n",
    "def get_and_save_area_codes():\n",
    "    html = get_html('https://hh.ru/search/vacancy?area=1347')\n",
    "    time.sleep(.3)\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    # areas_parsed = []\n",
    "\n",
    "    # нашли все объекты, которые содержат название региона и его код\n",
    "    pairs = soup.find('div', class_='clusters-group').find_all('a', class_='clusters-value')\n",
    "\n",
    "    # выделяем текст региона и кода, записываем в файл\n",
    "    with open('area_codes02.txt', 'w', encoding='utf-8') as f:\n",
    "        for pair in pairs:\n",
    "            area = pair.find('span', class_='clusters-value__name').get_text()\n",
    "            code = pair.get('href').split('&')[2].split('=')[1]\n",
    "            f.write(area+' '+code+'\\n')\n",
    "\n",
    "    print('DONE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce62fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gettin HTML-code from  https://tashkent.hh.uz/search/vacancy?text=frontend&area=2759&page=1\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/search/vacancy?text=frontend&area=2759&page=2\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/search/vacancy?text=frontend&area=2759&page=3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = 'frontend'\n",
    "area = '2759'\n",
    "# сначала вытащим все ссылки на вакансии по данному запросу и региону\n",
    "links = get_all_offers_links(query, area)\n",
    "# теперь распарсим информацию по каждой ссылке, полученной выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhjobs = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_title(soup):\n",
    "    title = soup.find('h1', data_qa_='vacancy-title')\n",
    "    text = ''.join(title.findAll(text=True))\n",
    "    return text\n",
    "\n",
    "def get_job_salary(soup):\n",
    "    title = soup.find('span', data_qa_='vacancy-salary-compensation-type-gross')\n",
    "    text = ''.join(title.findAll(text=True))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37fdeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/126824662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\AppData\\Local\\Temp\\ipykernel_11916\\1752254662.py:84: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n",
      "  text = ''.join(description.findAll(text=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/126966195\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/125981835\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/126994781\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/126906886\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/125205340\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/126488428\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/127260439\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/127617478\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/126842482\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/126801767\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/127251148\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/126898111\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/125721210\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/126736000\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/127090938\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/127618663\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/127484454\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/127127041\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/127262651\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/127264549\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/127933969\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/127407124\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/127616209\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/127431660\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/127534593\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/127061825\n",
      "Gettin HTML-code from  https://tashkent.hh.uz/vacancy/125673238\n"
     ]
    }
   ],
   "source": [
    "\n",
    "skill_dict = {}\n",
    "description_dict = {}\n",
    "for link in links:\n",
    "    html = get_html(link)\n",
    "    time.sleep(.3)\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    hhjobs[len(hhjobs)] = {'title':get_job_title(soup), 'salary':''}\n",
    "    skill_dict = parse_skills_in_offer(soup, skill_dict)\n",
    "    description_dict = parse_description_in_offer(soup, description_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f83e2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверено  28  вакансий.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# запишем навыки в файл skills_freq\n",
    "skills_sorted = sorted(skill_dict.items(), key=lambda x: x[1], reverse = True)\n",
    "with open('skill_freq.txt', 'w', encoding='utf-8') as f:\n",
    "    for skill in skills_sorted:\n",
    "        f.write(skill[0]+' '+str(skill[1])+'\\n')\n",
    "\n",
    "# запишем слова из описаний в файл descriptions_freq\n",
    "descriptions_sorted = sorted(description_dict.items(), key=lambda x: x[1], reverse = True)\n",
    "with open('description_freq.txt', 'w', encoding='utf-8') as f:\n",
    "    for description in descriptions_sorted:\n",
    "        f.write(description[0]+' '+str(description[1])+'\\n')\n",
    "print('Проверено ',len(links), ' вакансий.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
